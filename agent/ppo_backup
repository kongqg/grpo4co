"""
ppo.py

A readable, standard PPO implementation for Jumanji-style agents (JAX + Haiku),
written to be JIT/scan-friendly.

Key design points (important for correctness in JAX):
- Rollout data is collected as [T, B, ...] (time-major).
- GAE-Lambda advantages/returns are computed with a reverse lax.scan.
- Training uses flattened [N, ...] data where N = T * B.
- Minibatching uses permutation + padding + fixed-shape [num_minibatches, minibatch_size] indices,
  plus a mask to ignore padded elements. This avoids dynamic shapes in JIT/scan.

Constraint from user: `run_epoch` function name must remain unchanged.
"""

from __future__ import annotations

from typing import Any, Dict, Tuple

import jax
import jax.numpy as jnp
from chex import dataclass

from jumanji.training.agents.a2c import A2CAgent
from jumanji.training.types import ActingState


# -----------------------------------------------------------------------------
# Config
# -----------------------------------------------------------------------------

@dataclass
class PPOConfig:
    # PPO clipping
    clip_eps: float = 0.2
    # Value function loss weight
    vf_coef: float = 0.5
    # Entropy bonus weight
    ent_coef: float = 0.01
    # Number of PPO epochs per rollout
    update_epochs: int = 4
    # Minibatch size used for PPO updates
    minibatch_size: int = 64
    # Whether to normalize advantages across the rollout batch (recommended)
    normalize_adv: bool = True
    # If True, use clipped value loss (recommended for PPO)
    clip_value_loss: bool = True
    # Use separate value clip epsilon; if None, reuse clip_eps
    value_clip_eps: float | None = None
    policy_delay: int = 2


# -----------------------------------------------------------------------------
# Internal batch structures (PyTrees)
# -----------------------------------------------------------------------------

@dataclass
class RolloutTB:
    """Time-major rollout: leading dims [T, B]."""
    observation: Any                  # pytree with leaves [T, B, ...]
    raw_action: Any                   # pytree with leaves [T, B, ...] or [T, B]
    logp_old: jnp.ndarray             # [T, B]
    value_old: jnp.ndarray            # [T, B]
    reward: jnp.ndarray               # [T, B]
    discount: jnp.ndarray             # [T, B]  (0 at terminal, 1 otherwise)


@dataclass
class PPOBatchN:
    """Flattened rollout: leading dim [N] where N = T*B."""
    observation: Any                  # pytree with leaves [N, ...]
    raw_action: Any                   # pytree with leaves [N, ...] or [N]
    logp_old: jnp.ndarray             # [N]
    value_old: jnp.ndarray            # [N]
    advantage: jnp.ndarray            # [N]
    returns: jnp.ndarray              # [N]


# -----------------------------------------------------------------------------
# Utility helpers
# -----------------------------------------------------------------------------

def _maybe_squeeze_last(x: jnp.ndarray) -> jnp.ndarray:
    """Squeeze trailing singleton dimension if present (e.g., [B,1] -> [B])."""
    if hasattr(x, "ndim") and x.ndim > 0 and x.shape[-1] == 1:
        return jnp.squeeze(x, axis=-1)
    return x


def _extract_action_mask(observation: Any) -> Any | None:
    """Try to extract action mask from a Jumanji observation PyTree."""
    # common: dataclass/NamedTuple attribute
    if hasattr(observation, "action_mask"):
        return getattr(observation, "action_mask")
    # dict observation
    if isinstance(observation, dict) and "action_mask" in observation:
        return observation["action_mask"]
    return None


def _apply_action_mask_to_logits(logits: Any, mask: Any | None) -> Any:
    """Apply a boolean action mask to logits-like arrays (tree-friendly).

    For discrete actions: masked positions are set to a large negative number.
    If `mask` is None, logits are returned unchanged.
    """
    if mask is None:
        return logits

    def apply_one(logit_leaf, mask_leaf):
        if not isinstance(logit_leaf, jnp.ndarray) or not isinstance(mask_leaf, jnp.ndarray):
            return logit_leaf
        if mask_leaf.dtype != jnp.bool_:
            # treat non-bool masks as "no mask"
            return logit_leaf

        m = mask_leaf
        # Expand mask with trailing singleton dims until ranks match
        while m.ndim < logit_leaf.ndim:
            m = m[..., None]
        neg = jnp.array(-1e30, dtype=logit_leaf.dtype)
        return jnp.where(m, logit_leaf, neg)

    # If logits is a pytree, apply mask to every leaf; if mask isn't a pytree,
    # broadcast it to every leaf.
    if isinstance(logits, jnp.ndarray):
        return apply_one(logits, mask)

    if isinstance(mask, jnp.ndarray):
        return jax.tree_util.tree_map(lambda l: apply_one(l, mask), logits)
    return jax.tree_util.tree_map(apply_one, logits, mask)


def _tree_reshape_TB_to_N(tree: Any, T: int, B: int) -> Any:
    """Reshape leaves [T,B,...] -> [T*B,...] where applicable."""
    N = T * B

    def reshape_leaf(x):
        if not isinstance(x, jnp.ndarray):
            return x
        if x.ndim >= 2 and x.shape[0] == T and x.shape[1] == B:
            return x.reshape((N,) + x.shape[2:])
        return x

    return jax.tree_util.tree_map(reshape_leaf, tree)


def _masked_mean(x: jnp.ndarray, mask: jnp.ndarray) -> jnp.ndarray:
    """Compute mean of x over axis=0 with a {0,1} mask of same leading shape."""
    mask = mask.astype(x.dtype)
    denom = jnp.maximum(mask.sum(), 1.0)
    return (x * mask).sum() / denom


def _make_minibatch_indices(
    key: jax.Array, N: int, minibatch_size: int
) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """Return (idx, mb_mask):
    - idx: [num_minibatches, minibatch_size] with padded indices
    - mb_mask: [num_minibatches, minibatch_size] float32 mask for valid elements
    """
    num_minibatches = (N + minibatch_size - 1) // minibatch_size
    total = num_minibatches * minibatch_size

    perm = jax.random.permutation(key, N)  # [N]
    pad = total - N
    # Pad by wrapping around. Padded entries are masked out, so duplicates won't affect training.
    perm_padded = jnp.concatenate([perm, perm[:pad]], axis=0) if pad > 0 else perm
    idx = perm_padded.reshape((num_minibatches, minibatch_size))

    flat_mask = (jnp.arange(total) < N).astype(jnp.float32)
    mb_mask = flat_mask.reshape((num_minibatches, minibatch_size))
    return idx, mb_mask


def _compute_gae(
    rewards: jnp.ndarray,          # [T,B]
    discounts: jnp.ndarray,        # [T,B]
    values: jnp.ndarray,           # [T,B]
    last_value: jnp.ndarray,       # [B]
    gamma: float,
    lam: float,
) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """Standard GAE-Lambda (time-major). Returns (advantage, returns) both [T,B]."""
    next_values = jnp.concatenate([values[1:], last_value[None, ...]], axis=0)  # [T,B]
    deltas = rewards + gamma * discounts * next_values - values  # [T,B]

    def gae_scan(carry, xs):
        delta_t, discount_t = xs
        gae_t = delta_t + gamma * lam * discount_t * carry
        return gae_t, gae_t

    init = jnp.zeros_like(last_value)  # [B]
    _, adv_rev = jax.lax.scan(gae_scan, init, (deltas[::-1], discounts[::-1]))
    advantage = adv_rev[::-1]  # [T,B]
    returns = advantage + values
    return advantage, returns


# -----------------------------------------------------------------------------
# PPO Agent
# -----------------------------------------------------------------------------

class PPOAgent(A2CAgent):
    """A2CAgent-compatible PPO agent with standard GAE + minibatch shuffle."""

    def __init__(self, ppo_cfg: PPOConfig, **kwargs: Dict[str, Any]):
        super().__init__(**kwargs)
        self.ppo_cfg = ppo_cfg

    # -----------------------------
    # Rollout (collect [T,B,...])
    # -----------------------------
    def _rollout(self, params, acting_state: ActingState) -> Tuple[ActingState, RolloutTB, jnp.ndarray]:
        env = self.env
        dist = self.actor_critic_networks.parametric_action_distribution
        policy_apply = self.actor_critic_networks.policy_network.apply
        value_apply = self.actor_critic_networks.value_network.apply

        T = int(self.n_steps)

        def one_step(carry: ActingState, key: jax.Array):
            ts = carry.timestep
            obs = ts.observation

            logits = policy_apply(params.actor, obs)
            mask = _extract_action_mask(obs)
            logits = _apply_action_mask_to_logits(logits, mask)

            raw_action = dist.sample_no_postprocessing(logits, key)
            logp = dist.log_prob(logits, raw_action)
            action = dist.postprocess(raw_action)

            value = value_apply(params.critic, obs).astype(jnp.float32)
            value = _maybe_squeeze_last(value)

            next_state, next_ts = env.step(carry.state, action)

            # keep accounting consistent with A2C agent style (pmap-safe)
            new_acting_state = ActingState(
                state=next_state,
                timestep=next_ts,
                key=key,
                episode_count=carry.episode_count + jax.lax.psum(next_ts.last().sum(), "devices"),
                env_step_count=carry.env_step_count + jax.lax.psum(self.batch_size_per_device, "devices"),
            )

            reward = _maybe_squeeze_last(next_ts.reward).astype(jnp.float32)
            discount = _maybe_squeeze_last(next_ts.discount).astype(jnp.float32)

            transition = (obs, raw_action, logp, value, reward, discount)
            return new_acting_state, transition

        keys = jax.random.split(acting_state.key, T)  # [T,2]
        new_acting_state, traj = jax.lax.scan(one_step, acting_state, keys)

        obs_T, raw_action_T, logp_T, value_T, reward_T, discount_T = traj

        # Bootstrap at last observation after T steps
        last_obs = new_acting_state.timestep.observation
        last_value = value_apply(params.critic, last_obs).astype(jnp.float32)
        last_value = _maybe_squeeze_last(last_value)  # [B]

        rollout = RolloutTB(
            observation=obs_T,
            raw_action=raw_action_T,
            logp_old=logp_T.astype(jnp.float32),
            value_old=value_T.astype(jnp.float32),
            reward=reward_T.astype(jnp.float32),
            discount=discount_T.astype(jnp.float32),
        )
        return new_acting_state, rollout, last_value

    # -----------------------------
    # Build flat batch for PPO
    # -----------------------------
    def _build_batch(self, rollout: RolloutTB, last_value: jnp.ndarray) -> PPOBatchN:
        rewards = rollout.reward
        discounts = rollout.discount
        values = rollout.value_old

        # Pull gamma/lambda from A2C agent fields if present; otherwise fallback
        gamma = float(getattr(self, "discount_factor", 0.99))
        lam = float(getattr(self, "bootstrapping_factor", 0.95))

        adv, ret = _compute_gae(rewards, discounts, values, last_value, gamma=gamma, lam=lam)

        if self.ppo_cfg.normalize_adv:
            adv_flat = adv.reshape((-1,))
            adv_mean = adv_flat.mean()
            adv_std = adv_flat.std() + 1e-8
            adv = (adv - adv_mean) / adv_std

        # Flatten TB -> N
        T = int(rewards.shape[0])
        B = int(rewards.shape[1])
        obs_N = _tree_reshape_TB_to_N(rollout.observation, T, B)
        raw_action_N = _tree_reshape_TB_to_N(rollout.raw_action, T, B)

        logp_N = rollout.logp_old.reshape((T * B,))
        v_old_N = rollout.value_old.reshape((T * B,))
        adv_N = adv.reshape((T * B,))
        ret_N = ret.reshape((T * B,))

        return PPOBatchN(
            observation=obs_N,
            raw_action=raw_action_N,
            logp_old=jax.lax.stop_gradient(logp_N),
            value_old=jax.lax.stop_gradient(v_old_N),
            advantage=jax.lax.stop_gradient(adv_N),
            returns=jax.lax.stop_gradient(ret_N),
        )

    # -----------------------------
    # Loss on one minibatch
    # -----------------------------
    def _ppo_loss(
        self,
        params,
        batch_mb: PPOBatchN,            # each field has leading dim [M] where M=minibatch_size
        mb_mask: jnp.ndarray,           # [M] float32 {0,1}
        rng: jax.Array,
    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:
        dist = self.actor_critic_networks.parametric_action_distribution
        policy_apply = self.actor_critic_networks.policy_network.apply
        value_apply = self.actor_critic_networks.value_network.apply

        # Policy forward
        logits = policy_apply(params.actor, batch_mb.observation)
        mask = _extract_action_mask(batch_mb.observation)
        logits = _apply_action_mask_to_logits(logits, mask)

        logp_new = dist.log_prob(logits, batch_mb.raw_action).astype(jnp.float32)
        ratio = jnp.exp(logp_new - batch_mb.logp_old)

        eps = float(self.ppo_cfg.clip_eps)
        adv = batch_mb.advantage

        pg_loss_unclipped = ratio * adv
        pg_loss_clipped = jnp.clip(ratio, 1.0 - eps, 1.0 + eps) * adv
        pg_loss = -_masked_mean(jnp.minimum(pg_loss_unclipped, pg_loss_clipped), mb_mask)

        # Value forward
        v_new = value_apply(params.critic, batch_mb.observation).astype(jnp.float32)
        v_new = _maybe_squeeze_last(v_new)

        if self.ppo_cfg.clip_value_loss:
            v_eps = float(self.ppo_cfg.value_clip_eps) if self.ppo_cfg.value_clip_eps is not None else eps
            v_clipped = batch_mb.value_old + jnp.clip(v_new - batch_mb.value_old, -v_eps, v_eps)
            v_loss1 = jnp.square(v_new - batch_mb.returns)
            v_loss2 = jnp.square(v_clipped - batch_mb.returns)
            v_loss = 0.5 * _masked_mean(jnp.maximum(v_loss1, v_loss2), mb_mask)
        else:
            v_loss = 0.5 * _masked_mean(jnp.square(v_new - batch_mb.returns), mb_mask)

        # Entropy bonus
        rng, ent_key = jax.random.split(rng)
        entropy = dist.entropy(logits, ent_key).astype(jnp.float32)
        ent = _masked_mean(entropy, mb_mask)

        total_loss = pg_loss + self.ppo_cfg.vf_coef * v_loss - self.ppo_cfg.ent_coef * ent

        approx_kl = _masked_mean(batch_mb.logp_old - logp_new, mb_mask)
        clipfrac = _masked_mean((jnp.abs(ratio - 1.0) > eps).astype(jnp.float32), mb_mask)

        metrics = {
            "loss/total": total_loss,
            "loss/policy": pg_loss,
            "loss/value": v_loss,
            "stats/entropy": ent,
            "stats/approx_kl": approx_kl,
            "stats/clipfrac": clipfrac,
            "stats/adv_mean": _masked_mean(adv, mb_mask),
            "stats/ratio_mean": _masked_mean(ratio, mb_mask),
            "stats/value_mean": _masked_mean(v_new, mb_mask),
        }
        return total_loss, metrics

    # -----------------------------
    # Main API: run_epoch (do not rename)
    # -----------------------------
    def run_epoch(self, training_state):
        """One PPO iteration:
        1) rollout T steps on B envs
        2) compute GAE + returns
        3) PPO updates: update_epochs * num_minibatches with shuffle each epoch
        """
        params = training_state.params_state.params
        opt_state = training_state.params_state.opt_state
        update_count = training_state.params_state.update_count
        acting_state = training_state.acting_state

        # 1) rollout
        acting_state, rollout, last_value = self._rollout(params, acting_state)

        # 2) build flat batch
        batchN = self._build_batch(rollout, last_value)
        N = int(batchN.logp_old.shape[0])  # static python int

        mb = int(self.ppo_cfg.minibatch_size)
        if mb <= 0:
            mb = N  # full batch

        num_minibatches = (N + mb - 1) // mb  # python int, static
        update_epochs = int(self.ppo_cfg.update_epochs)

        # 3) PPO updates (JAX-friendly: fixed-shape minibatches via padding+mask)
        def one_epoch(carry, key_epoch):
            params, opt_state, key = carry
            key, perm_key, mbkey = jax.random.split(key, 3)

            idx_mat, mask_mat = _make_minibatch_indices(perm_key, N, mb)  # [M,mb], [M,mb]
            mb_keys = jax.random.split(mbkey, num_minibatches)

            def one_minibatch(carry2, xs):
                params, opt_state = carry2
                idx, m_mask, k = xs  # idx [mb], m_mask [mb]

                def _gather(tree):
                    def g(x):
                        if isinstance(x, jnp.ndarray) and x.ndim >= 1 and x.shape[0] == N:
                            return x[idx]
                        return x
                    return jax.tree_util.tree_map(g, tree)

                # Gather minibatch (fixed shape [mb,...])
                mbatch = PPOBatchN(
                    observation=_gather(batchN.observation),
                    raw_action=_gather(batchN.raw_action),
                    logp_old=batchN.logp_old[idx],
                    value_old=batchN.value_old[idx],
                    advantage=batchN.advantage[idx],
                    returns=batchN.returns[idx],
                )

                def loss_fn(p):
                    return self._ppo_loss(p, mbatch, m_mask, k)

                (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)

                updates, opt_state = self.optimizer.update(grads, opt_state, params)
                params = jax.tree_util.tree_map(lambda w, u: w + u, params, updates)

                # add grad norm metric (actor)
                actor_grads = jax.tree_util.tree_leaves(grads.actor)
                actor_gn = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in actor_grads if g is not None))
                metrics = dict(metrics)
                metrics["stats/grad_norm_actor"] = actor_gn
                return (params, opt_state), metrics

            (params, opt_state), metrics_seq = jax.lax.scan(
                one_minibatch, (params, opt_state), (idx_mat, mask_mat, mb_keys)
            )
            # return last minibatch metrics of this epoch
            metrics_last = jax.tree_util.tree_map(lambda x: x[-1], metrics_seq)
            return (params, opt_state, key), metrics_last

        # epoch keys and update rng
        key = acting_state.key
        key, epoch_key = jax.random.split(key)
        epoch_keys = jax.random.split(epoch_key, update_epochs)

        (params, opt_state, key), metrics_seq = jax.lax.scan(
            one_epoch, (params, opt_state, key), epoch_keys
        )
        # take last epoch's metrics (scalar dict)
        metrics = jax.tree_util.tree_map(lambda x: x[-1], metrics_seq)

        acting_state = acting_state._replace(key=key)

        new_params_state = training_state.params_state._replace(
            params=params,
            opt_state=opt_state,
            update_count=update_count + (update_epochs * num_minibatches),
        )
        new_state = training_state._replace(
            params_state=new_params_state,
            acting_state=acting_state,
        )
        return new_state, metrics
