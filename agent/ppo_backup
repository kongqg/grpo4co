# grpo_agent.py

from typing import Dict, Any

import jax
import jax.numpy as jnp
import rlax
from chex import dataclass
from jumanji.training.agents.a2c import A2CAgent
from jumanji.training.types import ActingState, Transition
from jumanji.types import StepType


@dataclass
class PPOConfig:
    clip_eps: float = 0.1
    normalize_adv: bool = True
    num_policy_updates: int = 3     # PPO epochs
    kl_coef: float = 0.0
    minibatch_size: int = 0         # 0/None => full batch


def _tree_true_like(x):
    return jax.tree.map(lambda t: jnp.ones_like(t, dtype=bool), x)


def _apply_mask_to_logits(logits, mask):
    def f(l, m):
        m = jnp.asarray(m, dtype=bool).reshape(l.shape)
        return jnp.where(m, l, jnp.full_like(l, -1e9))
    return jax.tree.map(f, logits, mask)


def _maybe_extract_action_mask(obs, logits):
    mask = None
    if hasattr(obs, "action_mask"):
        mask = obs.action_mask
    elif isinstance(obs, dict) and "action_mask" in obs:
        mask = obs["action_mask"]
    return jax.tree.map(lambda m: m.astype(bool), mask) if mask is not None else _tree_true_like(logits)


def _slice_transition_by_env(data: Transition, env_idx: jnp.ndarray, B: int) -> Transition:
    """沿 env/batch 维切分 Transition（[T,B,...] 切 axis=1，[B,...] 切 axis=0）。"""

    def slice_leaf(t):
        if not hasattr(t, "shape"):
            return t
        # [T, B, ...]
        if t.ndim >= 2 and t.shape[1] == B:
            return jnp.take(t, env_idx, axis=1)
        # [B, ...] 或 [B]
        if t.ndim >= 1 and t.shape[0] == B and (t.ndim == 1 or t.shape[1] != B):
            return jnp.take(t, env_idx, axis=0)
        return t

    return Transition(
        observation=jax.tree.map(slice_leaf, data.observation),
        action=jax.tree.map(slice_leaf, data.action),
        reward=slice_leaf(data.reward),
        discount=slice_leaf(data.discount),
        next_observation=jax.tree.map(slice_leaf, data.next_observation),
        log_prob=slice_leaf(data.log_prob),
        extras=jax.tree.map(slice_leaf, data.extras),
        logits=jax.tree.map(slice_leaf, data.logits),
    )


# ===================================================================
# PPO Agent（保持你的 rollout/丢弃末尾不完整 episode 逻辑不变）
# ===================================================================

class PPOAgent(A2CAgent):

    def __init__(self, grpo_cfg: PPOConfig, **kwargs: Dict[str, Any]):
        super().__init__(**kwargs)
        self.grpo_cfg = grpo_cfg
        self.batch_size = int(getattr(self, "total_batch_size", kwargs.get("total_batch_size")))

    def _loss_from_data(self, params, acting_state: ActingState, data: Transition):
        policy_apply = self.actor_critic_networks.policy_network.apply

        def fwd(_, ob):
            return None, policy_apply(params.actor, ob)

        _, new_logits = jax.lax.scan(fwd, None, data.observation)

        mask_b = data.extras.get("action_mask", _tree_true_like(new_logits))
        masked_new_logits = _apply_mask_to_logits(new_logits, mask_b)

        dist = self.actor_critic_networks.parametric_action_distribution
        # 用新概率选旧动作
        logp_new = dist.log_prob(masked_new_logits, data.extras.get("raw_action"))

        ratio = jnp.exp(logp_new - data.log_prob)
        advantage = data.extras.get("adv", None)
        targets = data.extras.get("targ", None)
        # weight = data.extras.get("valid_mask", jnp.ones_like(ratio))
        weight = jnp.ones_like(ratio, dtype=jnp.float32)
        clipped = jnp.clip(ratio, 1.0 - self.grpo_cfg.clip_eps, 1.0 + self.grpo_cfg.clip_eps)

        surr1 = ratio * jax.lax.stop_gradient(advantage)
        surr2 = clipped * jax.lax.stop_gradient(advantage)
        policy_loss = - (jnp.minimum(surr1, surr2) * weight).sum() / (weight.sum() + 1e-8)

        approx_kl = ((data.log_prob - logp_new) * weight).sum() / (weight.sum() + 1e-8)

        key, ent_key = jax.random.split(acting_state.key)
        entropy_t = dist.entropy(masked_new_logits, ent_key)
        entropy = (entropy_t * weight).sum() / (weight.sum() + 1e-8)
        entropy_loss = -entropy

        # -----------------------------
        # value_loss（标准 PPO：clipped value loss）
        # -----------------------------
        vapply = self.actor_critic_networks.value_network.apply

        def v_fwd(_, ob_t):
            v_new_t = vapply(params.critic, ob_t).astype(jnp.float32)
            return None, v_new_t

        _, value_new = jax.lax.scan(v_fwd, None, data.observation)  # [T, B]
        targets = jax.lax.stop_gradient(targets.astype(jnp.float32))  # [T, B]

        value_old = data.extras.get("value_old", None)
        value_old = jax.lax.stop_gradient(value_old.astype(jnp.float32))  # [T, B]
        v_clipped = value_old + jnp.clip(
            value_new - value_old,
            -self.grpo_cfg.clip_eps,
            self.grpo_cfg.clip_eps,
        )
        v_loss1 = (value_new - targets) ** 2
        v_loss2 = (v_clipped - targets) ** 2
        v_loss = jnp.maximum(v_loss1, v_loss2)
        value_loss = 0.5 * (v_loss * weight).sum() / (weight.sum() + 1e-8)


        total_loss = self.l_pg * policy_loss + self.l_td * value_loss + self.l_en * entropy_loss
        if self.grpo_cfg.kl_coef > 0.0:
            total_loss = total_loss + self.grpo_cfg.kl_coef * approx_kl

        metrics = dict(
            total_loss=total_loss,
            policy_loss=policy_loss,
            critic_loss=value_loss,
            entropy_loss=entropy_loss,
            entropy=entropy,
            advantage=advantage.mean(),
            value=value_new,
            approx_kl=approx_kl,
        )
        acting_state = acting_state._replace(key=key)
        return total_loss, (acting_state, metrics)

    def rollout_episodic(self, policy_params, value_params, acting_state: ActingState):
        T_max = self.n_steps
        env = self.env
        dist = self.actor_critic_networks.parametric_action_distribution
        policy_apply = self.actor_critic_networks.policy_network.apply
        key = acting_state.key
        B = self.batch_size

        key, reset_base = jax.random.split(key)
        base_keys = jax.random.split(reset_base, B)
        state, timestep = env.reset(base_keys)
        first_obs = timestep.observation
        B = jax.tree.leaves(first_obs)[0].shape[0]

        def step_fn(carry, _):
            key, state, obs_t = carry
            key, key_act, key_reset = jax.random.split(key, 3)

            env_ids = jnp.arange(B, dtype=jnp.uint32)
            actor_keys = jax.vmap(lambda k, i: jax.random.fold_in(k, i))(jax.random.split(key_act, B), env_ids)
            reset_keys = jax.vmap(lambda k, i: jax.random.fold_in(k, i))(jax.random.split(key_reset, B), env_ids)

            logits_t = policy_apply(policy_params, obs_t)
            mask_t = _maybe_extract_action_mask(obs_t, logits_t)
            masked_logits_t = _apply_mask_to_logits(logits_t, mask_t)

            raw_action_t = jax.vmap(dist.sample_no_postprocessing, in_axes=(0, 0))(masked_logits_t, actor_keys)
            logp_t = dist.log_prob(masked_logits_t, raw_action_t)

            v_t = self.actor_critic_networks.value_network.apply(value_params, obs_t).astype(jnp.float32)

            action_t = dist.postprocess(raw_action_t)
            next_state_raw, ts = env.step(state, action_t)

            reward_t = ts.reward
            discount_t = ts.discount
            next_obs_for_tr = ts.observation

            newly_done = (ts.step_type == StepType.LAST)  # [B]
            reset_state, reset_ts = env.reset(reset_keys)

            def blend(need_new, old_leaf, new_leaf):
                m = need_new
                for _ in range(new_leaf.ndim - m.ndim):
                    m = m[..., None]
                return jnp.where(m, new_leaf, old_leaf)

            next_state_carry = jax.tree.map(lambda a, b: blend(newly_done, a, b), next_state_raw, reset_state)
            next_obs_carry = jax.tree.map(lambda a, b: blend(newly_done, a, b), next_obs_for_tr, reset_ts.observation)

            transition = (
                obs_t, action_t, raw_action_t, reward_t, discount_t, next_obs_for_tr,
                logp_t, mask_t, logits_t, v_t
            )

            carry_out = (key, next_state_carry, next_obs_carry)
            return carry_out, transition

        init_carry = (key, state, first_obs)
        (last_key, _, last_obs), traj = jax.lax.scan(step_fn, init_carry, None, length=T_max)
        obs_b, action_b, raw_action_b, reward_b, discount_b, next_obs_b, logp_b, mask_b, logits_t, v_b = traj

        value_apply = self.actor_critic_networks.value_network.apply
        last_v_b = value_apply(value_params, last_obs).astype(jnp.float32)

        gamma = 0.99
        lam = 0.95

        v_old = v_b.astype(jnp.float32)  # [T,B]
        v_tp1 = jnp.concatenate([v_old[1:], last_v_b[None, ...]], axis=0)  # [T,B]
        r = reward_b.astype(jnp.float32)
        d = discount_b.astype(jnp.float32)  # 终止步=0，非终止=1

        # # 你原本的“丢弃最后一个未结束 episode”的逻辑：保持不动
        # is_done = (d == 0.0)  # [T, B]
        # done_inclusive_scan = jnp.cumsum(is_done[::-1], axis=0)[::-1]
        # valid_mask = (done_inclusive_scan > 0)
        # w = valid_mask.astype(jnp.float32)

        delta = (r + gamma * d * v_tp1 - v_old)

        def gae_scan(gae, xs):
            delta_t, d_t = xs
            gae = delta_t + gamma * lam * d_t * gae
            return gae, gae

        _, adv_rev = jax.lax.scan(gae_scan, jnp.zeros_like(last_v_b), (delta[::-1], d[::-1]))
        # advantage  = adv_rev[::-1] * w # [T,B]
        advantage = adv_rev[::-1]
        # advantage = raw_adv

        # if self.grpo_cfg.normalize_adv:
        #     den = jnp.maximum(w.sum(), 1.0)
        #     mu = (advantage * w).sum() / den
        #     var = (((advantage - mu) ** 2) * w).sum() / den
        #     sd = jnp.sqrt(var + 1e-8)
        #     advantage = (advantage - mu) / sd
        #     advantage = advantage * w
        if self.grpo_cfg.normalize_adv:
            mu = advantage.mean()
            sd = advantage.std() + 1e-8
            advantage = (advantage - mu) / sd

        targets = jax.lax.stop_gradient(advantage + v_old)  # [T,B]

        data = Transition(
            observation=obs_b,
            action=action_b,
            reward=reward_b,
            discount=discount_b,
            next_observation=next_obs_b,
            log_prob=logp_b,
            extras={
                "action_mask": mask_b,
                "value_old": v_b,           # [T,B]
                "last_value_old": last_v_b, # [B]
                "adv": advantage,           # [T,B]
                "targ": targets,            # [T,B]
                # "valid_mask": valid_mask,
                "raw_action": raw_action_b,
            },
            logits=logits_t,
        )
        new_acting_state = acting_state._replace(key=last_key)
        return new_acting_state, data

    def run_epoch(self, training_state):
        params = training_state.params_state.params
        opt_state = training_state.params_state.opt_state
        update_count = training_state.params_state.update_count
        acting_state = training_state.acting_state

        # 1) rollout
        acting_state, data = self.rollout_episodic(
            policy_params=params.actor,
            value_params=params.critic,
            acting_state=acting_state,
        )

        K = 3

        metrics = {}
        n_grad_steps = 0

        for _ in range(K):
            (loss, (acting_state, inner_metrics)), grads = jax.value_and_grad(self._loss_from_data, has_aux=True)(
                params, acting_state, data
            )

            #  L2 norm
            actor_grads_leaves = jax.tree_util.tree_leaves(grads.actor)
            actor_global_norm = jnp.sqrt(
                sum(jnp.sum(jnp.square(x)) for x in actor_grads_leaves if x is not None)
            )
            inner_metrics['grad_norm/actor_global_norm'] = actor_global_norm
            updates, opt_state = self.optimizer.update(grads, opt_state, params)
            params = jax.tree.map(lambda w, u: w + u, params, updates)
            metrics = inner_metrics

        new_params_state = training_state.params_state._replace(
            params=params,
            opt_state=opt_state,
            update_count=update_count + K,  # == update_count + K
        )
        new_state = training_state._replace(
            params_state=new_params_state,
            acting_state=acting_state,
        )
        return new_state, metrics
